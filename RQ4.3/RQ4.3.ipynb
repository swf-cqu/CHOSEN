{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import json\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def splitFromFile(category_name, sample_type):\r\n",
    "    ''' Get the Features and label from the JSON file\r\n",
    "\r\n",
    "    :param category_name: The path of Train or Test\r\n",
    "    :param sample_type: The concrete content of file\r\n",
    "    :return: data[] and target[]\r\n",
    "    '''\r\n",
    "\r\n",
    "    target = []\r\n",
    "    data = []\r\n",
    "    category_name=os.path.join(category_name,sample_type)\r\n",
    "    file_list = os.listdir(category_name)\r\n",
    "    file_list = sorted(file_list, key=lambda x: int(x[0:x.index('.')]))\r\n",
    "    for _ in file_list:\r\n",
    "        with open(os.path.join(category_name,_), encoding='UTF-8') as fp:\r\n",
    "            dic = json.load(fp)  # The size of content is only a line\r\n",
    "            a = [\r\n",
    "                  dic[\"add_annotation_line\"]\r\n",
    "                , dic[\"add_call_line\"]\r\n",
    "                , dic[\"add_classname_line\"]\r\n",
    "                , dic[\"add_condition_line\"]\r\n",
    "                , dic[\"add_field_line\"]\r\n",
    "                , dic[\"add_import_line\"]\r\n",
    "                , dic[\"add_packageid_line\"]\r\n",
    "                , dic[\"add_parameter_line\"]\r\n",
    "                , dic[\"add_return_line\"]\r\n",
    "                , dic[\"del_annotation_line\"]\r\n",
    "                , dic[\"del_call_line\"]\r\n",
    "                , dic[\"del_classname_line\"]\r\n",
    "                , dic[\"del_condition_line\"]\r\n",
    "                , dic[\"del_field_line\"]\r\n",
    "                , dic[\"del_import_line\"]\r\n",
    "                , dic[\"del_packageid_line\"]\r\n",
    "                , dic[\"del_parameter_line\"]\r\n",
    "                , dic[\"del_return_line\"]\r\n",
    "            ]\r\n",
    "            label = 0 if dic['label'] == \"NEGATIVE\" else 1\r\n",
    "            target.append(label)\r\n",
    "            data.append(a)\r\n",
    "            if sample_type==\"TestSample\" or sample_type==\"TestSample_Rule\":\r\n",
    "                if dic[\"true_label\"] is not None:\r\n",
    "                    target_true.append(0 if dic['true_label'] == \"NEGATIVE\" else 1)\r\n",
    "                else:\r\n",
    "                    raise Exception(\"The true_label is null\", _)\r\n",
    "    return data, target"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train(x_train,y_train):\r\n",
    "    #print(len(x_train), len(x_test))\r\n",
    "    model=RandomForestClassifier()\r\n",
    "    model.fit(x_train,y_train)\r\n",
    "    return model\r\n",
    "\r\n",
    "if __name__==\"__main__\":\r\n",
    "\r\n",
    "    target_true=[]\r\n",
    "\r\n",
    "    #启发规则过滤Training\r\n",
    "    project_path=r\"E:\\Project_Code\\JustInTestPro\\Heuristic_Sample\\commons-math\"\r\n",
    "    x_train_total,y_train_total=splitFromFile(project_path,\"Train\")\r\n",
    "    try:\r\n",
    "        x_test,y_test=splitFromFile(project_path,\"TestSample_Rule\")\r\n",
    "    except Exception as ep:\r\n",
    "        print(ep.__repr__())\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    preprocessing.scale(x_train_total)\r\n",
    "    preprocessing.scale(x_test)\r\n",
    "    total_sum=0\r\n",
    "    times=10\r\n",
    "    accuracyList1=[]\r\n",
    "    precisionList1=[]\r\n",
    "    recallList1=[]\r\n",
    "    accuracyList2=[]\r\n",
    "    precisionList2=[]\r\n",
    "    recallList2=[]\r\n",
    "    for i in range(times):\r\n",
    "        print(\"The times of iterator is %d\" %(i+1))\r\n",
    "        from imblearn.under_sampling import RandomUnderSampler\r\n",
    "        rus = RandomUnderSampler()\r\n",
    "        x_train, y_train = rus.fit_resample(x_train_total, y_train_total)\r\n",
    "        model=train(x_train,y_train)\r\n",
    "        y_pred=model.predict(x_test)\r\n",
    "        accuracyList1.append(accuracy_score(y_test, y_pred))\r\n",
    "        precisionList1.append(precision_score(y_test,y_pred))\r\n",
    "        recallList1.append(recall_score(y_test,y_pred))\r\n",
    "        accuracyList2.append(accuracy_score(target_true, y_pred))\r\n",
    "        precisionList2.append(precision_score(target_true,y_pred))\r\n",
    "        recallList2.append(recall_score(target_true,y_pred))\r\n",
    "    for i in zip(accuracyList2,precisionList2,recallList2):\r\n",
    "        print(\"The True Accuracy is {:.2%}; The True Precision is {:.2%}; The True Recall is {:.2%}\".format(\r\n",
    "            *i\r\n",
    "        ))\r\n",
    "    print(\"The average True Accuracy is {:.2%}; The True average Precision is {:.2%}; The True average Recall is {:.2%}\".format(\r\n",
    "        sum(accuracyList2)/times, sum(precisionList2)/times, sum(recallList2)/times)\r\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The times of iterator is 1\n",
      "The times of iterator is 2\n",
      "The times of iterator is 3\n",
      "The times of iterator is 4\n",
      "The times of iterator is 5\n",
      "The times of iterator is 6\n",
      "The times of iterator is 7\n",
      "The times of iterator is 8\n",
      "The times of iterator is 9\n",
      "The times of iterator is 10\n",
      "The True Accuracy is 75.28%; The True Precision is 65.99%; The True Recall is 68.17%\n",
      "The True Accuracy is 75.28%; The True Precision is 65.45%; The True Recall is 69.97%\n",
      "The True Accuracy is 72.95%; The True Precision is 62.33%; The True Recall is 67.57%\n",
      "The True Accuracy is 74.94%; The True Precision is 64.58%; The True Recall is 71.17%\n",
      "The True Accuracy is 73.28%; The True Precision is 63.14%; The True Recall is 66.37%\n",
      "The True Accuracy is 74.17%; The True Precision is 63.66%; The True Recall is 69.97%\n",
      "The True Accuracy is 74.61%; The True Precision is 64.29%; The True Recall is 70.27%\n",
      "The True Accuracy is 74.83%; The True Precision is 65.14%; The True Recall is 68.47%\n",
      "The True Accuracy is 73.39%; The True Precision is 64.05%; The True Recall is 63.66%\n",
      "The True Accuracy is 75.06%; The True Precision is 65.34%; The True Recall is 69.07%\n",
      "The average True Accuracy is 74.38%; The True average Precision is 64.40%; The True average Recall is 68.47%\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit (system)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "0e9fc4577d07dc28dbd47afd814ee57ac7a26fdf49fcf109372d1cc24e657e70"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}